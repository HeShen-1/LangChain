from langchain_community.document_loaders import TextLoader, PyPDFLoader, CSVLoader, Docx2txtLoader
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import tempfile
import os

try:
    from langchain_huggingface import HuggingFaceEmbeddings
    HUGGINGFACE_AVAILABLE = True
except ImportError:
    HUGGINGFACE_AVAILABLE = False


def get_embed(openai_api_key=None, force_backup=False):
    """è·å–è¯åµŒå…¥æ¨¡å‹"""
    import os
    
    # å¦‚æœå¼ºåˆ¶ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆï¼Œç›´æ¥è·³è¿‡OpenAI
    if force_backup:
        print("ğŸ”„ å¼ºåˆ¶ä½¿ç”¨å¤‡é€‰åµŒå…¥æ–¹æ¡ˆ")
    elif openai_api_key:
        try:
            print("ğŸ”„ å°è¯•ä½¿ç”¨OpenAIåµŒå…¥API")
            embed_model = OpenAIEmbeddings(
                api_key=openai_api_key,
                base_url='https://twapi.openai-hk.com/v1/'
            )
            # è¿›è¡Œæ›´ä¸¥æ ¼çš„APIæµ‹è¯• - æµ‹è¯•å¤šæ¬¡ç¡®ä¿ç¨³å®šæ€§
            print("ğŸ§ª æ­£åœ¨æµ‹è¯•OpenAIåµŒå…¥API...")
            for i in range(3):  # æµ‹è¯•3æ¬¡
                test_result = embed_model.embed_query(f"æµ‹è¯•æ–‡æœ¬{i+1}")
                if not test_result or not isinstance(test_result, list) or len(test_result) == 0:
                    print(f"âŒ OpenAIåµŒå…¥APIç¬¬{i+1}æ¬¡æµ‹è¯•å¤±è´¥")
                    raise ValueError(f"OpenAI APIç¬¬{i+1}æ¬¡æµ‹è¯•å“åº”æ— æ•ˆ")
            print("âœ… OpenAIåµŒå…¥APIå¤šæ¬¡æµ‹è¯•æˆåŠŸ")
            return embed_model
        except Exception as e:
            print(f"âŒ OpenAIåµŒå…¥APIå¤±è´¥: {e}")
            print("ğŸ”„ åˆ‡æ¢åˆ°å¤‡é€‰åµŒå…¥æ–¹æ¡ˆ...")
    
    # ç«‹å³ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆï¼Œé¿å…ç½‘ç»œå»¶è¿Ÿ
    print("ğŸ”„ ä½¿ç”¨å¤‡é€‰åµŒå…¥æ–¹æ¡ˆ...")
    
    # ä¼˜å…ˆä½¿ç”¨å‡åµŒå…¥æ¨¡å‹ï¼ˆå¿«é€Ÿå¯é ï¼‰
    try:
        print("âš¡ ä½¿ç”¨å¿«é€ŸåµŒå…¥æ¨¡å‹ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
        from langchain_community.embeddings import FakeEmbeddings
        return FakeEmbeddings(size=384)
    except Exception as e:
        print(f"âŒ å‡åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
    
    # å¦‚æœå‡åµŒå…¥ä¸å¯ç”¨ï¼Œå°è¯•HuggingFaceæ¨¡å‹
    if HUGGINGFACE_AVAILABLE:
        try:
            # å°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆé¿å…ç½‘ç»œè¯·æ±‚ï¼‰
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            model_path = os.path.join(project_root, "paraphrase-multilingual-Mpnet-base-v2")
            
            if os.path.exists(model_path):
                print(f"ğŸ”„ å°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}")
                embed_model = HuggingFaceEmbeddings(
                    model_name=model_path,
                    model_kwargs={'device': 'cpu'},
                    encode_kwargs={'normalize_embeddings': False}
                )
                return embed_model
        except Exception as e:
            print(f"âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        try:
            # æœ€åå°è¯•åœ¨çº¿æ¨¡å‹
            print("ğŸ”„ å°è¯•åœ¨çº¿åµŒå…¥æ¨¡å‹: all-MiniLM-L6-v2")
            embed_model = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': False}
            )
            return embed_model
        except Exception as e:
            print(f"âŒ åœ¨çº¿æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    raise RuntimeError("æ— æ³•åŠ è½½ä»»ä½•åµŒå…¥æ¨¡å‹ã€‚è¯·æ£€æŸ¥APIå¯†é’¥æˆ–ç½‘ç»œè¿æ¥ã€‚")


def load_documents(files):
    """æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½æ–‡æ¡£"""
    documents = []

    for file in files:
        file_ext = os.path.splitext(file.name)[1].lower()
        temp_dir = tempfile.mkdtemp()
        temp_path = os.path.join(temp_dir, file.name)

        # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
        with open(temp_path, "wb") as f:
            f.write(file.getbuffer())

        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
            if file_ext == '.pdf':
                loader = PyPDFLoader(temp_path)
            elif file_ext == '.txt':
                loader = TextLoader(temp_path, encoding='utf-8')
            elif file_ext == '.csv':
                loader = CSVLoader(temp_path)
            elif file_ext == '.docx':
                loader = Docx2txtLoader(temp_path)
            else:
                print(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.name}")
                continue

            # åŠ è½½æ–‡æ¡£
            docs = loader.load()
            # æ·»åŠ æ¥æºä¿¡æ¯
            for doc in docs:
                doc.metadata['source'] = file.name

            documents.extend(docs)
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_path)
            os.rmdir(temp_dir)

    return documents


def qa_agent(openai_api_key, memory, uploaded_files, question):
    """
    å¤šæ–‡ä»¶é—®ç­”åŠ©æ‰‹æ ¸å¿ƒå‡½æ•°
    :param openai_api_key: OpenAI APIå¯†é’¥
    :param memory: å†å²å¯¹è¯å†…å­˜
    :param uploaded_files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
    :param question: ç”¨æˆ·æå‡ºçš„é—®é¢˜
    :return: å›ç­”åŠæ¥æºæ–‡æ¡£
    """
    if not uploaded_files:
        return {"answer": "è¯·å…ˆä¸Šä¼ æ–‡ä»¶", "source_documents": []}

    try:
        model = ChatOpenAI(
            model='gpt-3.5-turbo',
            api_key=openai_api_key,
            base_url='https://twapi.openai-hk.com/v1/'
        )

        # åŠ è½½æ–‡æ¡£
        docs = load_documents(uploaded_files)
        if not docs:
            return {"answer": "æ–‡æ¡£åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼", "source_documents": []}

        # æ–‡æœ¬åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=250,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ",", "ã€"]
        )
        texts = text_splitter.split_documents(docs)
        
        if not texts:
            return {"answer": "æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è§£æ", "source_documents": []}

        # è¯åµŒå…¥æ¨¡å‹
        print("ğŸ”„ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
        embedding_model = None
        use_backup_embedding = False
        
        try:
            embedding_model = get_embed(openai_api_key)
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # ç›´æ¥ä½¿ç”¨å¤‡é€‰åµŒå…¥æ¨¡å‹
            try:
                print("ğŸ”„ ä½¿ç”¨å¤‡é€‰åµŒå…¥æ¨¡å‹...")
                embedding_model = get_embed(openai_api_key, force_backup=True)
                use_backup_embedding = True
                print("âœ… å¤‡é€‰åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as backup_e:
                return {"answer": f"æ‰€æœ‰åµŒå…¥æ¨¡å‹éƒ½åŠ è½½å¤±è´¥: {str(e)}, å¤‡é€‰: {str(backup_e)}", "source_documents": []}

        # åˆ›å»ºå‘é‡æ•°æ®åº“
        print("ğŸ”„ æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
        db = None
        retriever = None
        
        try:
            # ä¸ºäº†é¿å…å¤§é‡æ–‡æ¡£é€ æˆçš„é—®é¢˜ï¼Œé™åˆ¶æ–‡æ¡£æ•°é‡
            if len(texts) > 100:
                texts = texts[:100]  # åªä½¿ç”¨å‰100ä¸ªæ–‡æ¡£ç‰‡æ®µ
                print(f"âš ï¸ æ–‡æ¡£ç‰‡æ®µè¿‡å¤šï¼Œåªä½¿ç”¨å‰100ä¸ªç‰‡æ®µ")
            
            db = FAISS.from_documents(texts, embedding_model)
            print("âœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ")
            # å°†å‘é‡æ•°æ®åº“è½¬æ¢ä¸ºæ£€ç´¢å™¨
            retriever = db.as_retriever()
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥: {e}")
            # å¦‚æœè¿˜æ²¡æœ‰ä½¿ç”¨å¤‡é€‰åµŒå…¥ï¼Œç°åœ¨å°è¯•
            if not use_backup_embedding:
                try:
                    print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡é€‰åµŒå…¥æ¨¡å‹é‡å»ºæ•°æ®åº“...")
                    backup_embedding = get_embed(openai_api_key, force_backup=True)
                    db = FAISS.from_documents(texts, backup_embedding)
                    retriever = db.as_retriever()
                    print("âœ… ä½¿ç”¨å¤‡é€‰åµŒå…¥æ¨¡å‹é‡å»ºæ•°æ®åº“æˆåŠŸ")
                except Exception as backup_e:
                    return {"answer": f"å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥: {str(e)}\nå¤‡é€‰æ–¹æ¡ˆä¹Ÿå¤±è´¥: {str(backup_e)}", "source_documents": []}
            else:
                return {"answer": f"å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥: {str(e)}", "source_documents": []}

        # åˆ›å»ºé—®ç­”é“¾å¹¶å¤„ç†æŸ¥è¯¢
        try:
            qa = ConversationalRetrievalChain.from_llm(
                llm=model,
                retriever=retriever,
                memory=memory,
                return_source_documents=True,
            )
            print("ğŸ”„ æ­£åœ¨å¤„ç†é—®ç­”...")
            # è°ƒç”¨é—®ç­”é“¾ï¼Œå¤„ç†ç”¨æˆ·é—®é¢˜
            response = qa.invoke({
                'chat_history': memory.buffer,
                'question': question
            })
            print("âœ… é—®ç­”å¤„ç†æˆåŠŸ")
            return response
        except Exception as e:
            print(f"âŒ é—®ç­”å¤„ç†å¤±è´¥: {e}")
            # å¦‚æœé—®ç­”å¤„ç†å¤±è´¥ï¼Œå¯èƒ½æ˜¯æŸ¥è¯¢æ—¶åµŒå…¥æ¨¡å‹é—®é¢˜ï¼Œå°è¯•é‡å»ºæ•´ä¸ªç³»ç»Ÿ
            if not use_backup_embedding:
                try:
                    print("ğŸ”„ æ£€æµ‹åˆ°æŸ¥è¯¢é˜¶æ®µåµŒå…¥é—®é¢˜ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆé‡å»º...")
                    backup_embedding = get_embed(openai_api_key, force_backup=True)
                    db_backup = FAISS.from_documents(texts, backup_embedding)
                    retriever_backup = db_backup.as_retriever()
                    
                    qa_backup = ConversationalRetrievalChain.from_llm(
                        llm=model,
                        retriever=retriever_backup,
                        memory=memory,
                        return_source_documents=True,
                    )
                    response = qa_backup.invoke({
                        'chat_history': memory.buffer,
                        'question': question
                    })
                    print("âœ… å¤‡é€‰æ–¹æ¡ˆé—®ç­”å¤„ç†æˆåŠŸ")
                    return response
                except Exception as backup_e:
                    return {"answer": f"é—®ç­”å¤„ç†å¤±è´¥: {str(e)}\nå¤‡é€‰æ–¹æ¡ˆä¹Ÿå¤±è´¥: {str(backup_e)}", "source_documents": []}
            else:
                return {"answer": f"é—®ç­”å¤„ç†å¤±è´¥: {str(e)}", "source_documents": []}
            
    except Exception as e:
        return {"answer": f"ç³»ç»Ÿé”™è¯¯: {str(e)}", "source_documents": []}


def gen_followup_questions(question, answer, openai_api_key):
    """
    æ ¹æ®å½“å‰é—®é¢˜å’Œç­”æ¡ˆç”Ÿæˆä¸‰ä¸ªåç»­å¯èƒ½çš„æé—®
    """
    if not openai_api_key:
        return []
        
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚ç”¨æˆ·åˆšåˆšé—®äº†è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼š\n"
        f"é—®é¢˜ï¼š{question}\n"
        f"AIçš„å›ç­”ï¼š{answer}\n"
        "è¯·åŸºäºç”¨æˆ·çš„åŸå§‹é—®é¢˜å’ŒAIçš„å›ç­”ï¼Œç”Ÿæˆä¸‰ä¸ªç”¨æˆ·å¯èƒ½ä¼šç»§ç»­è¿½é—®çš„ç›¸å…³é—®é¢˜ã€‚"
        "åªè¿”å›é—®é¢˜æœ¬èº«ï¼Œæ¯ä¸ªé—®é¢˜ä¸€è¡Œï¼Œä¸è¦ç¼–å·ã€‚"
    )
    # è¿™é‡Œç”¨OpenAIæ¥å£ç”Ÿæˆé—®é¢˜ï¼ˆå¦‚éœ€æ›´æ¢æ¨¡å‹å¯è‡ªè¡Œè°ƒæ•´ï¼‰
    from langchain_openai import ChatOpenAI
    try:
        model = ChatOpenAI(
            model='gpt-3.5-turbo',
            api_key=openai_api_key,
            base_url='https://twapi.openai-hk.com/v1/'
        )
        resp = model.invoke([{"role": "user", "content": prompt}])
        # è§£æè¿”å›çš„æ–‡æœ¬ä¸ºé—®é¢˜åˆ—è¡¨
        questions = [line.strip() for line in resp.content.split('\n') if line.strip()]
        return questions[:3]
    except Exception as e:
        print(f"ç”Ÿæˆåç»­é—®é¢˜å¤±è´¥: {e}")
        return []

