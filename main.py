import streamlit as st
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, AIMessage
from utils import  qa_agent, gen_followup_questions

# ä½¿ç”¨ä¸€ä¸ªåˆ—è¡¨ç¼“å­˜å†å²è®°å½•
if 'history_cache' not in st.session_state:
    st.session_state['history_cache'] = []

st.set_page_config(page_title="å¤šæ–‡ä»¶æ™ºèƒ½é—®ç­”åŠ©æ‰‹", layout="wide")

# ====== é¡¶éƒ¨é…ç½®åŒºåŸŸ ======
st.markdown("### ğŸ¤– æ™ºèƒ½é—®ç­”åŠ©æ‰‹é…ç½®åŒº")
col1, col2 = st.columns([2, 1])  # å·¦ä¾§2ä»½å®½åº¦ï¼Œå³ä¾§1ä»½å®½åº¦

with col1:
    openai_key = st.text_input('è¯·è¾“å…¥OpenAI APIå¯†é’¥', type='password')
    st.markdown('[è·å–OpenAI APIç§˜é’¥](https://openai-hk.com/v3/ai/key)')

with col2:
    st.markdown("### æ”¯æŒçš„æ–‡ä»¶ç±»å‹")
    st.info("PDF, TXT, CSV, DOCX")
    upload_files = st.file_uploader(
        "ä¸Šä¼ æ–‡ä»¶",
        type=["pdf", "txt", "csv", "docx"],
        accept_multiple_files=True
    )

# ====== æ“ä½œæŒ‰é’®åŒºåŸŸ ======
st.markdown("---")  # åˆ†éš”çº¿
col1, col2 = st.columns([1, 1])

with col1:
    # æ–°å»ºå¯¹è¯æŒ‰é’®
    if st.button("æ–°å»ºå¯¹è¯"):
        st.session_state['memory'] = ConversationBufferMemory(
            return_messages=True,
            memory_key='chat_history',
            output_key='answer'
        )
        st.session_state['chat_history'] = []
        st.session_state['followup_questions'] = []
        st.session_state['last_question'] = ""
        st.session_state['user_input'] = ""
        st.success("æ–°å¯¹è¯å·²å¼€å§‹ï¼")

        # æ–°ä¼šè¯å¼€å§‹åä¿å­˜åˆ°å†å²è®°å½•
        session_data = {
            'memory': st.session_state['memory'],
            'chat_history': st.session_state['chat_history'],
            'followup_questions': st.session_state['followup_questions'],
            'last_question': st.session_state['last_question']
        }
        st.session_state['history_cache'].append(session_data)

with col2:
    # å†å²è®°å½•åŒºåŸŸ
    st.markdown("### å†å²è®°å½•")
    if st.session_state['history_cache']:
        # ä½¿ç”¨ä¸‹æ‹‰æ¡†é€‰æ‹©å†å²è®°å½•ï¼ˆæ›´èŠ‚çœç©ºé—´ï¼‰
        history_idx = st.selectbox(
            "é€‰æ‹©å†å²å¯¹è¯",
            list(range(1, len(st.session_state['history_cache']) + 1))
        )
        if st.button(f"åŠ è½½ å†å²å¯¹è¯ {history_idx}"):
            history = st.session_state['history_cache'][history_idx - 1]
            st.session_state['memory'] = history['memory']
            st.session_state['chat_history'] = history['chat_history']
            st.session_state['followup_questions'] = history['followup_questions']
            st.session_state['last_question'] = history['last_question']
            st.session_state['user_input'] = ""
            st.success(f"å·²åŠ è½½ å†å²å¯¹è¯ {history_idx}")
    else:
        st.info("æ²¡æœ‰å†å²è®°å½•ã€‚")

# ====== ä¼šè¯çŠ¶æ€åˆå§‹åŒ– ======
if 'memory' not in st.session_state:
    st.session_state['memory'] = ConversationBufferMemory(
        return_messages=True,
        memory_key='chat_history',
        output_key='answer'
    )
if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = []
if 'followup_questions' not in st.session_state:
    st.session_state['followup_questions'] = []
if 'last_question' not in st.session_state:
    st.session_state['last_question'] = ""
if 'user_input' not in st.session_state:
    st.session_state['user_input'] = ""

# ====== ä¸»åŒºåŸŸæ ·å¼ ======
st.markdown(
    """
    <style>
    .fixed-bottom-bar {
        position: fixed;
        left: 0;
        right: 0;
        bottom: 0;
        background: #fff;
        padding: 1rem 0.5rem 0.5rem 1rem;
        box-shadow: 0 -2px 8px rgba(0,0,0,0.04);
        z-index: 100;
    }
    .followup-btn {
        margin-right: 8px;
        margin-bottom: 8px;
        display: inline-block;
        font-size: 14px;
        color: #007bff;
        background-color: #fff;
        border: 1px solid #007bff;
        border-radius: 4px;
        padding: 5px 10px;
        cursor: pointer;
        transition: all 0.3s;
    }
    .followup-btn:hover {
        background-color: #007bff;
        color: white;
    }
    .chat-container {
        padding-bottom: 200px; /* ä¸ºåº•éƒ¨å›ºå®šåŒºåŸŸç•™å‡ºç©ºé—´ */
        padding-top: 20px; /* å¢åŠ é¡¶éƒ¨é—´è· */
    }
    .message-box {
        margin-top: 20px;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("å­¦è€Œä¸æ€åˆ™ç½”ï¼Œæ€è€Œä¸å­¦åˆ™æ®†")

# ====== èŠå¤©å†å²å±•ç¤º ======
chat_container = st.container()
chat_container.markdown('<div class="chat-container">', unsafe_allow_html=True)

# ä½¿ç”¨ st.chat_message é‡æ„èŠå¤©ç•Œé¢
chat_history = st.session_state['memory'].load_memory_variables({}).get('chat_history', [])
for message in chat_history:
    if isinstance(message, HumanMessage):
        with chat_container:
            st.chat_message("user").markdown(message.content)
    elif isinstance(message, AIMessage):
        with chat_container:
            ai = message.content
            st.chat_message("assistant").markdown(ai)

            # å±•ç¤ºæ¥æº (æ•´åˆåˆ°AIæ¶ˆæ¯ä¸­)
            if hasattr(message, 'source_documents') and message.source_documents:
                with st.expander('å›ç­”æ¥æº'):
                    sources = {}
                    for doc in message.source_documents:
                        source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
                        if source not in sources:
                            sources[source] = []
                        sources[source].append(doc.page_content)
                    for source, contents in sources.items():
                        if "ä¸çŸ¥é“" in ai:
                            st.markdown(f"æ‰¾ä¸åˆ°æ¥æº")
                            continue
                        st.markdown(f"**æ¥æº: {source}**")
                        st.markdown(f"ç‰‡æ®µ : {contents[0]}")

chat_container.markdown('</div>', unsafe_allow_html=True)

# ====== å›ºå®šåº•éƒ¨è¾“å…¥åŒº ======
st.markdown('<div class="fixed-bottom-bar">', unsafe_allow_html=True)

# é¢„æµ‹é—®é¢˜æŒ‰é’®åŒº
cols = st.columns(3)
for idx, q in enumerate(st.session_state['followup_questions']):
    if cols[idx].button(q, key=f"followup_{idx}", help="ç‚¹å‡»å¡«å…¥è¾“å…¥æ¡†"):
        st.session_state['user_input'] = q

# ç”¨st.chat_inputç¾åŒ–è¾“å…¥æ¡†
user_input = st.chat_input(
    "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
    key="user_input_box",
    disabled=not upload_files
)

st.markdown('</div>', unsafe_allow_html=True)

# ====== å¤„ç†æé—® ======
if upload_files and user_input:
    if not openai_key:
        st.info('è¯·è¾“å…¥ä½ çš„OpenAI APIå¯†é’¥')
        st.stop()
    question = user_input
    with st.spinner("AIæ­£åœ¨æ€è€ƒä¸­..."):
        response = qa_agent(
            openai_api_key=openai_key,
            memory=st.session_state['memory'],
            uploaded_files=upload_files,
            question=question
        )
    # ä¸å†æ‰‹åŠ¨è¿½åŠ chat_historyï¼Œäº¤ç”±memoryç»´æŠ¤
    st.session_state['last_question'] = question

    st.session_state['followup_questions'] = gen_followup_questions(
        question=question,
        answer=response['answer'],
        openai_api_key=openai_key
    )

    st.session_state["user_input"] = ""

    # å°†æ¥æºä¿¡æ¯æ·»åŠ åˆ°AIæ¶ˆæ¯ä¸­
    if response['source_documents']:
        # æ‰¾åˆ°æœ€æ–°çš„AIæ¶ˆæ¯å¹¶æ·»åŠ æ¥æºä¿¡æ¯
        messages = st.session_state['memory'].load_memory_variables({}).get('chat_history', [])
        if messages and isinstance(messages[-1], AIMessage):
            messages[-1].source_documents = response['source_documents']

    st.rerun()